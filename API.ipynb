{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 5 - Catégorisez automatiquement des questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des librairies et des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "import csv\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34802,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation de la base initiale\n",
    "df = pd.read_csv('filtered_df.csv', sep = ';', index_col = 0)\n",
    "df.fillna(' ', inplace = True)\n",
    "df['Body'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des tags\n",
    "tags = np.load('tags.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de tous les modèles issus de l'analyse du projet\n",
    "model_final = joblib.load('model_final.plk')\n",
    "pca = joblib.load('model_pca.plk')\n",
    "std_scale = joblib.load('std_scale.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88373"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des stopwords\n",
    "list_sw = []\n",
    "with open('list_stop_words.csv', 'r') as data:\n",
    "    for line in csv.reader(data):\n",
    "        list_sw.append(line)\n",
    "\n",
    "    # Transforme la liste de listes en liste\n",
    "sw = [item for sublist in list_sw for item in sublist]\n",
    "stop_words = set(sw)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des données et du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning, tokenizing and lemmatizing\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "        return [self.wnl.lemmatize(t) for t in token.tokenize(doc.lower()) if t not in stop_words]\n",
    "wnl = WordNetLemmatizer()    \n",
    "tokenizer = lambda x: [wnl.lemmatize(x) for x in token.tokenize(x.lower()) if x not in stop_words]\n",
    "\n",
    "\n",
    "#Tf-Idf\n",
    "count = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                        stop_words=stop_words, analyzer='word')\n",
    "tfidf = TfidfTransformer()\n",
    "pipe = make_pipeline(count, tfidf)\n",
    "\n",
    "\n",
    "pipe.fit(df['Body'])\n",
    "X_features = pipe.transform(df['Body'])\n",
    "feature_names = count.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrée de l'utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Interesting download error when trying to download html canvas as image with button\n"
     ]
    }
   ],
   "source": [
    "# User's title input\n",
    "title = input(\"Title: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body: When I first click the button it doesn't work. On my second click it downloads 1 picture. My 3rd click it downloads 2 pictures. On my 4th click it downloads 3 pictures. So 1-0, 2-1, 3-2, 4-3. They are also downloaded immediately, it doesn't ask where to save.  js:    function xyz(){   const text =canvas.api.getCanvasAsImage();   const download = document.getElementById('download');   download.addEventListener('click', function(e) {   var link = document.createElement('a');   link.download = 'download.png';   link.href = text;   link.click();   link.delete; }); } html:  <button  onclick=\"xyz()\" id=\"download\">Download</button> I have just started learning javascript. I'm trying to learn by examining an application. I did not understand why these is happening and therefore could not solve the problem.\n"
     ]
    }
   ],
   "source": [
    "# User's body imput\n",
    "body = input(\"Body: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage du tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ruby-on-rails']\n"
     ]
    }
   ],
   "source": [
    "# Jointure du titre et du corps pour créer la question\n",
    "question = title + \" \" + body\n",
    "\n",
    "# Créer un dataframe contenant la question\n",
    "df_question = pd.DataFrame({'Question': [question]})\n",
    "\n",
    "# Cleaning and processing\n",
    "features = pipe.transform(df_question['Question'])\n",
    "mx_feature = pd.DataFrame(features.toarray(), columns = feature_names)\n",
    "std_features = std_scale.transform(mx_feature)\n",
    "acp = pca.transform(std_features)\n",
    "\n",
    "# Prediction\n",
    "predicted_tags = model_final.predict(acp)\n",
    "\n",
    "# Affichage du tag\n",
    "print(tags[predicted_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
